# CloudGuard AI - Infrastructure Risk Scanner

> **âš ï¸ PROPRIETARY SOFTWARE - ALL RIGHTS RESERVED**  
> **Â© 2025 chavanarya36**  
> This repository is available for **viewing and educational purposes only**.  
> Redistribution, modification, commercial use, or claiming credit is **strictly prohibited**.  
> See [LICENSE](LICENSE) for full terms. Unauthorized use will result in legal action.

---

AI-powered Infrastructure-as-Code (IaC) security scanner using machine learning to detect potential security vulnerabilities.

## ğŸ¯ Features

- **Single File Analysis**: Upload individual .tf, .yaml, .json, or .bicep files for risk assessment
- **Batch Processing**: Upload ZIP archives containing multiple IaC files for bulk analysis
- **Real-time Risk Scoring**: Get probability scores and binary risk decisions
- **Detailed Explanations**: Understand why files are flagged as risky
- **Professional UI**: Clean, intuitive interface with dark theme
- **Configurable Thresholds**: Adjust risk sensitivity based on your needs
- **Export Results**: Download analysis results in CSV or JSON format



---```- **Real-time Risk Scoring**: Get probability scores and binary risk decisions



## ğŸ“ Project StructureCloudGuardAI/- **Detailed Explanations**: Understand why files are flagged as risky



```â”œâ”€â”€ app.py                      # Streamlit web application- **Professional UI**: Clean, intuitive interface suitable for enterprise use

CloudGuardAI/

â”œâ”€â”€ app.py                      # Streamlit web applicationâ”œâ”€â”€ data/                       # Data files and datasets- **Configurable Thresholds**: Adjust risk sensitivity based on your needs

â”œâ”€â”€ README.md                   # This file

â”œâ”€â”€ data/                       # Data files and datasetsâ”‚   â”œâ”€â”€ iac_labels_clean.csv    # Labeled IaC security findings- **Export Results**: Download analysis results in CSV or JSON format

â”‚   â”œâ”€â”€ iac_labels_clean.csv

â”‚   â”œâ”€â”€ programs.csvâ”‚   â”œâ”€â”€ iac_labels_summary.csv  # Label statistics summary

â”‚   â””â”€â”€ repositories.csv

â”œâ”€â”€ pipeline/                   # ML pipeline scriptsâ”‚   â”œâ”€â”€ programs.csv            # IaC program inventory## Installation

â”‚   â”œâ”€â”€ 01_prepare_labels.py

â”‚   â”œâ”€â”€ 02_build_features.pyâ”‚   â”œâ”€â”€ repositories.csv        # GitHub repository list

â”‚   â”œâ”€â”€ 03_train_model.py

â”‚   â””â”€â”€ ... (8 more scripts)â”‚   â””â”€â”€ merged_findings_v2_sample.csv### Local Development

â”œâ”€â”€ scanners/                   # IaC scanner integration

â”‚   â”œâ”€â”€ scan_checkov.pyâ”œâ”€â”€ pipeline/                   # ML pipeline scripts

â”‚   â”œâ”€â”€ scan_tfsec.py

â”‚   â””â”€â”€ scan_kics.pyâ”‚   â”œâ”€â”€ 01_prepare_labels.py1. Clone the repository:

â”œâ”€â”€ features_artifacts/         # Extracted ML features

â”œâ”€â”€ models_artifacts/           # Trained modelsâ”‚   â”œâ”€â”€ 02_build_features.py```bash

â”œâ”€â”€ predictions_artifacts/      # Model predictions

â”œâ”€â”€ utils/                      # Utility functionsâ”‚   â”œâ”€â”€ 03_train_model.pygit clone <repository-url>

â”‚   â”œâ”€â”€ model_loader.py

â”‚   â”œâ”€â”€ feature_extractor.pyâ”‚   â”œâ”€â”€ 04_predict_and_rank.pycd CloudGuardAI

â”‚   â””â”€â”€ prediction_engine.py

â”œâ”€â”€ docs/                       # Documentationâ”‚   â”œâ”€â”€ 05_leakage_sanity.py```

â”‚   â”œâ”€â”€ README.md

â”‚   â”œâ”€â”€ README_pipeline.mdâ”‚   â”œâ”€â”€ 05_validation_sanity.py

â”‚   â””â”€â”€ model_report.md

â””â”€â”€ config/                     # Configurationâ”‚   â”œâ”€â”€ 06_reliability_diagnostics.py2. Install dependencies:

    â”œâ”€â”€ requirements.txt

    â”œâ”€â”€ Dockerfileâ”‚   â”œâ”€â”€ 07_threshold_tuning.py```bash

    â””â”€â”€ run_full_pipeline.ps1

```â”‚   â”œâ”€â”€ 08_per_repo_validation.pypip install -r requirements.txt



---â”‚   â””â”€â”€ summarize_metrics.py```



## ğŸš€ Quick Startâ”œâ”€â”€ scanners/                   # IaC scanner integration



### 1. Install Dependenciesâ”‚   â”œâ”€â”€ scan_checkov.py3. Ensure model artifacts are present:



```bashâ”‚   â”œâ”€â”€ scan_tfsec.py- `models_artifacts/best_model_lr.joblib`

pip install -r config/requirements.txt

```â”‚   â”œâ”€â”€ scan_kics.py- `models_artifacts/threshold_lr.json`



### 2. Run Web Applicationâ”‚   â”œâ”€â”€ merge_findings.py- `models_artifacts/cv_metrics_lr.json`



```bashâ”‚   â””â”€â”€ *_outputs/              # Scanner results- `features_artifacts/meta.json`

streamlit run app.py

# orâ”œâ”€â”€ features_artifacts/         # Extracted ML features

python -m streamlit run app.py

```â”œâ”€â”€ models_artifacts/           # Trained models4. Run the application:



### 3. Open Browserâ”œâ”€â”€ predictions_artifacts/      # Model predictions```bash



Navigate to `http://localhost:8501`â”œâ”€â”€ labels_artifacts/           # Processed labelsstreamlit run app.py



---â”œâ”€â”€ utils/                      # Utility functions```



## ğŸ³ Docker Deploymentâ”œâ”€â”€ scripts/                    # Helper scripts



### Build Imageâ”œâ”€â”€ tests/                      # Test files5. Open your browser to `http://localhost:8501`



```bashâ”œâ”€â”€ docs/                       # Documentation

docker build -t cloudguard-ai -f config/Dockerfile .

```â”‚   â”œâ”€â”€ README.md               # Main documentation### Docker Deployment



### Run Containerâ”‚   â”œâ”€â”€ README_pipeline.md      # Pipeline guide



```bashâ”‚   â”œâ”€â”€ README_PROJECT.md       # Project overview1. Build the Docker image:

docker run -p 8501:8501 cloudguard-ai

```â”‚   â””â”€â”€ model_report.md         # Model performance```bash



Access at `http://localhost:8501`â””â”€â”€ config/                     # Configurationdocker build -t cloudguard-ai .



---    â”œâ”€â”€ requirements.txt```



## ğŸ’» Usage    â”œâ”€â”€ Dockerfile



### Single File Mode    â””â”€â”€ run_full_pipeline.ps12. Run the container:



1. Select "ğŸ“„ Single File Analysis" in the sidebar``````bash

2. Upload a supported IaC file (.tf, .yaml, .yml, .json, .bicep)

3. Click "ğŸ” Analyze File Security"docker run -p 8501:8501 cloudguard-ai

4. View the risk assessment, gauge, and detailed explanation

5. Adjust threshold if needed using the sidebar slider---```



### Batch Mode



1. Select "ğŸ“¦ Batch Processing" in the sidebar## ğŸš€ Quick Start3. Access the application at `http://localhost:8501`

2. Upload a ZIP file containing your IaC files

3. Click "ğŸ” Analyze ZIP Archive"

4. Review summary metrics and detailed results

5. Use filters to find specific risk levels### 1. Install Dependencies## Usage

6. Download results as CSV or JSON

```bash

---

pip install -r config/requirements.txt### Single File Mode

## ğŸ§  Model Information

```1. Select "Single File" in the sidebar

- **Algorithm**: Logistic Regression (liblinear solver, L2 regularization)

- **Features**: 32,768 sparse hash features + 8 dense structural features2. Upload a supported IaC file (.tf, .yaml, .yml, .json, .bicep)

- **Performance**: 

  - PR-AUC: 0.3379### 2. Run Web Application3. View the risk assessment and explanation

  - ROC-AUC: 0.9726

  - Balanced Accuracy: 0.9500```bash4. Adjust threshold if needed using the sidebar slider

- **Training Data**: 21,107 IaC files with 2.3% positive rate

- **Calibration**: 5-fold Sigmoid calibrationstreamlit run app.py



See [Model Report](docs/model_report.md) for detailed performance metrics.```### Batch Mode



---1. Select "Batch Upload" in the sidebar



## ğŸ”§ Configuration### 3. Run Full Pipeline2. Upload a ZIP file containing your IaC files



### Model Artifacts Required```bash3. Click "Analyze ZIP Archive"



The application requires these files:python pipeline/01_prepare_labels.py4. Review the summary metrics and detailed results table

- `models_artifacts/best_model_lr.joblib` - Trained scikit-learn model

- `models_artifacts/threshold_lr.json` - Global decision thresholdpython pipeline/02_build_features.py5. Download results as CSV or JSON

- `models_artifacts/cv_metrics_lr.json` - Model performance metrics

- `features_artifacts/meta.json` - Feature extraction metadatapython pipeline/03_train_model.py



### Supported File Types```## Model Information



- **Terraform**: `.tf` files

- **YAML**: `.yaml`, `.yml` files (Kubernetes, Docker Compose, etc.)

- **JSON**: `.json` files (CloudFormation, etc.)---- **Algorithm**: Logistic Regression (liblinear solver)

- **Bicep**: `.bicep` files (Azure Resource Manager)

- **Features**: Sparse hash features from file paths and content, plus dense structural features

---

## ğŸ“š Documentation- **Performance**: PR-AUC â‰ˆ 0.34, ROC-AUC â‰ˆ 0.97

## ğŸ“š Documentation

- **Training Data**: 21,107 IaC files with 2.3% positive rate

- **[Main Documentation](docs/README.md)** - Comprehensive project guide

- **[Pipeline Guide](docs/README_pipeline.md)** - ML pipeline details- **[Main Documentation](docs/README.md)** - Comprehensive project guide

- **[Project Overview](docs/README_PROJECT.md)** - Architecture and design

- **[Model Report](docs/model_report.md)** - Performance metrics and analysis- **[Pipeline Guide](docs/README_pipeline.md)** - ML pipeline details## Architecture



---- **[Project Overview](docs/README_PROJECT.md)** - Architecture and design



## ğŸ› ï¸ Development- **[Model Report](docs/model_report.md)** - Performance metrics```



### Run ML Pipelineapp.py                          # Main Streamlit application



```bash---utils/

# Prepare labels

python pipeline/01_prepare_labels.pyâ”œâ”€â”€ __init__.py



# Build features## ğŸ”§ Configurationâ”œâ”€â”€ model_loader.py            # Load trained model and artifacts

python pipeline/02_build_features.py

â”œâ”€â”€ feature_extractor.py       # Extract features from IaC files

# Train model

python pipeline/03_train_model.pyConfiguration files located in `config/`:â””â”€â”€ prediction_engine.py       # Handle predictions and batch processing



# Make predictions- `requirements.txt` - Python dependenciesmodels_artifacts/              # Trained model files

python pipeline/04_predict_and_rank.py

```- `Dockerfile` - Container configurationfeatures_artifacts/            # Feature metadata



### Run Tests- `run_full_pipeline.ps1` - Automated pipeline executionrequirements.txt               # Python dependencies



```bashDockerfile                     # Container configuration

pytest tests/

```---```



### Run Scanners



```bash## ğŸ“Š Key Features## API Reference

# Checkov scanner

python scanners/scan_checkov.py



# tfsec scannerâœ… Multi-scanner integration (Checkov, tfsec, KICS)  ### PredictionEngine

python scanners/scan_tfsec.py

âœ… Machine learning-based vulnerability prioritization  

# KICS scanner

python scanners/scan_kics.pyâœ… Interactive web interface with Streamlit  Main class for handling predictions:

```

âœ… Comprehensive reliability diagnostics  

---

âœ… Per-repository validation  ```python

## ğŸ“Š Performance

âœ… Threshold tuning for precision/recall optimization  from utils.prediction_engine import PredictionEngine

- **Single file analysis**: < 1 second

- **Batch processing**: ~100 files per second

- **Memory usage**: ~200MB base + ~1MB per 1000 files

---engine = PredictionEngine()

---



## ğŸ”’ Security Considerations

## ğŸ“ˆ Model Performance# Single file prediction

- Files are processed in memory without persistent storage

- Temporary files are automatically cleaned up after processingresult = engine.predict_single_file(file_path, content)

- No data is transmitted outside the application

- Suitable for air-gapped environments- **PR-AUC**: 0.3379

- All processing happens locally

- **ROC-AUC**: 0.9726# Batch prediction

---

- **Dataset**: 21,107 labeled IaC filesresults = engine.predict_batch(file_data_list)

## ğŸ› Troubleshooting

- **Positive cases**: 490 (2.3%)

### Model Not Found Error

Ensure all model artifacts are present in `models_artifacts/` and `features_artifacts/` directories.# Process ZIP file



### Feature Extraction ErrorsSee [Model Report](docs/model_report.md) for detailed metrics.results = engine.process_zip_file(zip_path)

Check that uploaded files are valid IaC formats and properly encoded (UTF-8).

```

### Memory Errors on Large Batches

Process smaller ZIP files or increase available memory. Docker users can use `--memory` flag.---



### Streamlit Command Not Found### ModelLoader

Use `python -m streamlit run app.py` instead of `streamlit run app.py`.

## ğŸ› ï¸ Development

---

Load and manage ML artifacts:

## ğŸ“ License

### Run Tests

[Add your license information here]

```bash```python

---

pytest tests/from utils.model_loader import ModelLoader

**Last Updated**: October 31, 2025  

**Version**: 1.0  ```

**Status**: âœ… Production Ready

loader = ModelLoader()

### Rebuild Featuresmodel, threshold, metrics = loader.load_all()

```bash```

python pipeline/02_build_features.py

```### FeatureExtractor



### Retrain ModelExtract features from IaC files:

```bash

python pipeline/03_train_model.py```python

```from utils.feature_extractor import FeatureExtractor



---extractor = FeatureExtractor()

X, feature_info = extractor.extract_features_single(file_path, content)

## ğŸ“ License```



[Your License Here]## Configuration



---### Environment Variables



**Last Updated**: October 31, 2025  - `STREAMLIT_SERVER_PORT`: Port for the web application (default: 8501)

**Version**: 1.0  - `STREAMLIT_SERVER_ADDRESS`: Server address (default: 0.0.0.0 for Docker)

**Status**: Production Ready âœ…

### Model Artifacts

The application requires these files to be present:

- `models_artifacts/best_model_lr.joblib`: Trained scikit-learn model
- `models_artifacts/threshold_lr.json`: Global decision threshold
- `models_artifacts/cv_metrics_lr.json`: Model performance metrics
- `features_artifacts/meta.json`: Feature extraction metadata

## Supported File Types

- **Terraform**: `.tf` files
- **YAML**: `.yaml`, `.yml` files (Kubernetes, Docker Compose, etc.)
- **JSON**: `.json` files (CloudFormation, etc.)
- **Bicep**: `.bicep` files (Azure Resource Manager)

## Performance

- Single file analysis: < 1 second
- Batch processing: ~100 files per second (depends on file size)
- Memory usage: ~200MB base + ~1MB per 1000 files in batch

## Security Considerations

- Files are processed in memory without persistent storage
- Temporary files are automatically cleaned up
- No data is transmitted outside the application
- Suitable for air-gapped environments

## Troubleshooting

### Common Issues

1. **Model not found error**: Ensure all model artifacts are present in the correct directories
2. **Feature extraction errors**: Check that uploaded files are valid IaC formats
3. **Memory errors on large batches**: Process smaller ZIP files or increase container memory

### Logs

When running with Docker, view logs with:
```bash
docker logs <container-id>
```

## Contributing

1. Follow the existing code structure
2. Add tests for new features
3. Update documentation as needed
4. Ensure Docker build succeeds

## License

[Add your license information here]